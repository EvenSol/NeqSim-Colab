{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Integration with NeqSim\n",
    "\n",
    "This notebook demonstrates how **NeqSim** can be embedded into reinforcement learning (RL) workflows for process control and optimization.\n",
    "\n",
    "- [Reinforcement learning (Wikipedia)](https://en.wikipedia.org/wiki/Reinforcement_learning)\n",
    "- [OpenAI Gym](https://www.gymlibrary.dev/)\n",
    "- [Introductory RL video](https://www.youtube.com/watch?v=2pWv7GOvuf0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Birth of Reinforcement Learning\n",
    "\n",
    "The origins of RL trace back to early work on trial-and-error learning in the 1950s and the formalization of temporal-difference methods by Sutton and Barto in the 1980s.\n",
    "The goal is to learn a policy $\\pi$ that maximizes the expected discounted reward\n",
    "\n",
    "$$ J(\\pi) = \\mathbb{E}_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\right] $$\n",
    "\n",
    "where $\\gamma$ is a discount factor and $r_t$ is the reward at time $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding NeqSim in an RL Environment\n",
    "\n",
    "By wrapping NeqSim simulations inside an [OpenAI Gym](https://www.gymlibrary.dev/) interface, agents can interact with a simulated process to learn control policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%capture\n",
    "!pip install neqsim gym matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from neqsim import jneqsim\n",
    "\n",
    "class NeqSimValveEnv(gym.Env):\n",
    "    \"\"\"Simple valve pressure control environment using NeqSim.\"\"\"\n",
    "    metadata = {'render.modes': []}\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Box(low=0.0, high=100.0, shape=(1,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0.0, high=10.0, shape=(1,), dtype=np.float32)\n",
    "        self.target = 2.0\n",
    "        fluid = jneqsim.thermo.system.SystemSrkEos(298.15, 10.0)\n",
    "        fluid.addComponent('methane',0.9)\n",
    "        fluid.addComponent('ethane',0.1)\n",
    "        fluid.addComponent('n-heptane',1.0)\n",
    "        fluid.setMixingRule('classic')\n",
    "        feed = jneqsim.process.equipment.stream.Stream('Feed', fluid)\n",
    "        feed.setFlowRate(50.0, 'kg/hr')\n",
    "        feed.setPressure(10.0, 'bara')\n",
    "        self.valve = jneqsim.process.equipment.valve.ThrottlingValve('valve', feed)\n",
    "        self.valve.setOutletPressure(1.0)\n",
    "        self.valve.setCalculateSteadyState(False)\n",
    "        self.trans = jneqsim.process.measurementdevice.PressureTransmitter(self.valve.getOutletStream())\n",
    "        self.trans.setUnit('bar')\n",
    "        self.process = jneqsim.process.processmodel.ProcessSystem()\n",
    "        self.process.add(feed); self.process.add(self.valve); self.process.add(self.trans)\n",
    "        self.reset()\n",
    "    def step(self, action):\n",
    "        self.valve.setPercentValveOpening(float(action[0]))\n",
    "        self.process.run()\n",
    "        p = self.trans.getMeasuredValue()\n",
    "        reward = -(p - self.target)**2\n",
    "        obs = np.array([p], dtype=np.float32)\n",
    "        done = False\n",
    "        return obs, reward, done, {}\n",
    "    def reset(self):\n",
    "        self.valve.setPercentValveOpening(50.0)\n",
    "        self.process.run()\n",
    "        p = self.trans.getMeasuredValue()\n",
    "        return np.array([p], dtype=np.float32)\n",
    "\n",
    "# Demonstrate random interaction\n",
    "env = NeqSimValveEnv()\n",
    "obs = env.reset()\n",
    "for _ in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(f'pressure {obs[0]:.2f} bar, reward {reward:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Design with Thermodynamic Insights\n",
    "\n",
    "NeqSim's detailed thermodynamic calculations enable reward functions that capture operational objectives such as energy efficiency, cost, and safety. A generic reward can be expressed as\n",
    "\n",
    "$$ r = -\\alpha E - \\beta C + \\gamma S $$\n",
    "\n",
    "where $E$ is energy usage, $C$ is operating cost, and $S$ is a safety metric computed from NeqSim outputs. Selecting suitable weights $\\alpha$, $\\beta$, and $\\gamma$ guides the RL agent toward efficient and safe operation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}